"""
Property-based tests for schedule distribution output.

Tests Property 13: Schedule Distribution Output
Validates: Requirements 5.4

These tests verify that the Monte Carlo Engine produces probability distributions 
showing completion likelihood by target dates.
"""

import pytest
import numpy as np
from datetime import datetime, timedelta
from hypothesis import given, strategies as st, assume, settings
from hypothesis.extra.numpy import arrays

from monte_carlo.distribution_outputs import (
    DistributionOutputGenerator, ScheduleComplianceResult, DistributionSummary
)
from monte_carlo.models import SimulationResults, ConvergenceMetrics


class TestScheduleDistributionOutputProperties:
    """Property-based tests for schedule distribution output functionality."""
    
    @given(
        schedule_outcomes=arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1000, max_value=10000),
            elements=st.floats(min_value=30.0, max_value=1000.0, allow_nan=False, allow_infinity=False)
        ),
        target_days_offset=st.integers(min_value=-100, max_value=500)
    )
    @settings(max_examples=50, deadline=5000)
    def test_completion_probability_bounds(self, schedule_outcomes, target_days_offset):
        """
        Property: Completion probability should be between 0 and 1.
        
        This property ensures that completion probability calculations always
        produce valid probability values regardless of input distributions.
        """
        assume(len(schedule_outcomes) > 0)
        assume(np.all(np.isfinite(schedule_outcomes)))
        assume(np.all(schedule_outcomes > 0))
        
        # Create dates
        project_start_date = datetime(2024, 1, 1)
        mean_duration = float(np.mean(schedule_outcomes))
        target_date = project_start_date + timedelta(days=mean_duration + target_days_offset)
        
        # Create simulation results
        simulation_results = self._create_simulation_results(schedule_outcomes)
        
        generator = DistributionOutputGenerator()
        result = generator.calculate_schedule_compliance(
            simulation_results, target_date, project_start_date
        )
        
        # Property: Completion probability must be between 0 and 1
        assert 0.0 <= result.completion_probability <= 1.0
        
        # Property: Expected completion should be a valid date
        assert isinstance(result.expected_completion, datetime)
        assert result.expected_completion >= project_start_date
        
        # Property: Schedule at risk should be non-negative
        assert result.schedule_at_risk >= 0
    
    @given(
        schedule_outcomes=arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1000, max_value=5000),
            elements=st.floats(min_value=30.0, max_value=500.0, allow_nan=False, allow_infinity=False)
        ),
        target_offset_1=st.integers(min_value=-50, max_value=100),
        target_offset_2=st.integers(min_value=101, max_value=300)
    )
    @settings(max_examples=50, deadline=5000)
    def test_completion_probability_monotonicity(self, schedule_outcomes, target_offset_1, target_offset_2):
        """
        Property: Later target dates should result in higher completion probabilities.
        
        This property ensures that completion probability increases monotonically
        with target date, which is fundamental to schedule analysis.
        """
        assume(len(schedule_outcomes) > 0)
        assume(np.all(np.isfinite(schedule_outcomes)))
        assume(np.all(schedule_outcomes > 0))
        assume(target_offset_2 > target_offset_1)
        
        # Create dates
        project_start_date = datetime(2024, 1, 1)
        mean_duration = float(np.mean(schedule_outcomes))
        
        target_date_1 = project_start_date + timedelta(days=mean_duration + target_offset_1)
        target_date_2 = project_start_date + timedelta(days=mean_duration + target_offset_2)
        
        # Create simulation results
        simulation_results = self._create_simulation_results(schedule_outcomes)
        
        generator = DistributionOutputGenerator()
        
        result_1 = generator.calculate_schedule_compliance(
            simulation_results, target_date_1, project_start_date
        )
        result_2 = generator.calculate_schedule_compliance(
            simulation_results, target_date_2, project_start_date
        )
        
        # Property: Later target should have higher or equal completion probability
        assert result_2.completion_probability >= result_1.completion_probability
        
        # Property: Schedule at risk should decrease with later target
        assert result_2.schedule_at_risk <= result_1.schedule_at_risk
    
    @given(
        schedule_outcomes=arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1000, max_value=5000),
            elements=st.floats(min_value=30.0, max_value=500.0, allow_nan=False, allow_infinity=False)
        )
    )
    @settings(max_examples=50, deadline=5000)
    def test_confidence_interval_nesting(self, schedule_outcomes):
        """
        Property: Confidence intervals should be properly nested.
        
        This property ensures that higher confidence levels produce wider intervals
        that contain lower confidence level intervals.
        """
        assume(len(schedule_outcomes) > 0)
        assume(np.all(np.isfinite(schedule_outcomes)))
        assume(np.all(schedule_outcomes > 0))
        
        # Create dates
        project_start_date = datetime(2024, 1, 1)
        mean_duration = float(np.mean(schedule_outcomes))
        target_date = project_start_date + timedelta(days=mean_duration + 50)
        
        # Create simulation results
        simulation_results = self._create_simulation_results(schedule_outcomes)
        
        generator = DistributionOutputGenerator()
        result = generator.calculate_schedule_compliance(
            simulation_results, target_date, project_start_date, 
            confidence_levels=[0.80, 0.90, 0.95]
        )
        
        # Property: Confidence intervals should be nested
        ci_80 = result.confidence_intervals['80%']
        ci_90 = result.confidence_intervals['90%']
        ci_95 = result.confidence_intervals['95%']
        
        # 95% CI should contain 90% CI, which should contain 80% CI
        assert ci_95[0] <= ci_90[0] <= ci_80[0]  # Earlier dates (lower bounds)
        assert ci_80[1] <= ci_90[1] <= ci_95[1]  # Later dates (upper bounds)
        
        # Property: All confidence intervals should have positive duration
        assert ci_80[1] > ci_80[0]
        assert ci_90[1] > ci_90[0]
        assert ci_95[1] > ci_95[0]
        
        # Property: All dates should be after project start
        for ci in [ci_80, ci_90, ci_95]:
            assert ci[0] >= project_start_date
            assert ci[1] >= project_start_date
    
    @given(
        schedule_outcomes=arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1000, max_value=5000),
            elements=st.floats(min_value=30.0, max_value=500.0, allow_nan=False, allow_infinity=False)
        )
    )
    @settings(max_examples=50, deadline=5000)
    def test_percentile_ordering(self, schedule_outcomes):
        """
        Property: Percentiles should be properly ordered.
        
        This property ensures that percentile calculations produce monotonically
        increasing completion dates, which is fundamental to distribution analysis.
        """
        assume(len(schedule_outcomes) > 0)
        assume(np.all(np.isfinite(schedule_outcomes)))
        assume(np.all(schedule_outcomes > 0))
        
        # Create dates
        project_start_date = datetime(2024, 1, 1)
        mean_duration = float(np.mean(schedule_outcomes))
        target_date = project_start_date + timedelta(days=mean_duration + 50)
        
        # Create simulation results
        simulation_results = self._create_simulation_results(schedule_outcomes)
        
        generator = DistributionOutputGenerator()
        result = generator.calculate_schedule_compliance(
            simulation_results, target_date, project_start_date
        )
        
        # Property: Percentiles should be ordered
        percentiles = result.percentile_analysis
        p_dates = [percentiles[f'P{p}'] for p in [10, 25, 50, 75, 90, 95, 99]]
        
        for i in range(len(p_dates) - 1):
            assert p_dates[i] <= p_dates[i + 1]
        
        # Property: All percentile dates should be after project start
        for p_date in p_dates:
            assert p_date >= project_start_date
        
        # Property: Median (P50) should be close to expected completion
        expected_completion = result.expected_completion
        median_completion = percentiles['P50']
        
        # Allow reasonable tolerance for median vs mean difference
        time_diff = abs((median_completion - expected_completion).total_seconds())
        max_allowed_diff = mean_duration * 24 * 3600 * 0.5  # 50% of mean duration in seconds
        assert time_diff <= max_allowed_diff
    
    @given(
        schedule_outcomes=arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1000, max_value=5000),
            elements=st.floats(min_value=30.0, max_value=500.0, allow_nan=False, allow_infinity=False)
        ),
        num_milestones=st.integers(min_value=1, max_value=5)
    )
    @settings(max_examples=50, deadline=5000)
    def test_milestone_probability_consistency(self, schedule_outcomes, num_milestones):
        """
        Property: Milestone completion probabilities should be consistent with overall schedule.
        
        This property ensures that milestone probabilities are logically consistent
        with the overall project completion probability.
        """
        assume(len(schedule_outcomes) > 0)
        assume(np.all(np.isfinite(schedule_outcomes)))
        assume(np.all(schedule_outcomes > 0))
        
        # Create dates
        project_start_date = datetime(2024, 1, 1)
        mean_duration = float(np.mean(schedule_outcomes))
        target_date = project_start_date + timedelta(days=mean_duration + 50)
        
        # Create milestone dates (evenly spaced before target)
        milestone_dates = {}
        for i in range(num_milestones):
            milestone_day = mean_duration * (i + 1) / (num_milestones + 1)
            milestone_date = project_start_date + timedelta(days=milestone_day)
            milestone_dates[f'Milestone_{i+1}'] = milestone_date
        
        # Create simulation results
        simulation_results = self._create_simulation_results(schedule_outcomes)
        
        generator = DistributionOutputGenerator()
        result = generator.calculate_schedule_compliance(
            simulation_results, target_date, project_start_date,
            milestone_dates=milestone_dates
        )
        
        # Property: Milestone probabilities should exist
        assert result.milestone_probabilities is not None
        assert len(result.milestone_probabilities) == num_milestones
        
        # Property: All milestone probabilities should be between 0 and 1
        for milestone_prob in result.milestone_probabilities.values():
            assert 0.0 <= milestone_prob <= 1.0
        
        # Property: Earlier milestones should have higher completion probabilities
        milestone_items = sorted(
            [(name, date, prob) for name, date in milestone_dates.items() 
             for prob in [result.milestone_probabilities[name]]],
            key=lambda x: x[1]  # Sort by date
        )
        
        for i in range(len(milestone_items) - 1):
            current_prob = milestone_items[i][2]
            next_prob = milestone_items[i + 1][2]
            # Earlier milestone should have higher or equal probability
            assert current_prob >= next_prob - 0.01  # Small tolerance for numerical precision
    
    @given(
        schedule_outcomes=arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1000, max_value=5000),
            elements=st.floats(min_value=30.0, max_value=500.0, allow_nan=False, allow_infinity=False)
        )
    )
    @settings(max_examples=50, deadline=5000)
    def test_extreme_target_scenarios(self, schedule_outcomes):
        """
        Property: Extreme target scenarios should produce expected completion results.
        
        This property tests edge cases with very early and very late target dates
        to ensure robust behavior.
        """
        assume(len(schedule_outcomes) > 0)
        assume(np.all(np.isfinite(schedule_outcomes)))
        assume(np.all(schedule_outcomes > 0))
        
        # Create dates
        project_start_date = datetime(2024, 1, 1)
        mean_duration = float(np.mean(schedule_outcomes))
        
        # Create simulation results
        simulation_results = self._create_simulation_results(schedule_outcomes)
        generator = DistributionOutputGenerator()
        
        # Test very early target (should have ~0% completion probability)
        very_early_target = project_start_date + timedelta(days=mean_duration * 0.1)
        early_result = generator.calculate_schedule_compliance(
            simulation_results, very_early_target, project_start_date
        )
        
        # Property: Very early target should have low completion probability
        assert early_result.completion_probability <= 0.1
        assert early_result.schedule_at_risk >= mean_duration * 0.5  # High risk
        
        # Test very late target (should have ~100% completion probability)
        very_late_target = project_start_date + timedelta(days=mean_duration * 3.0)
        late_result = generator.calculate_schedule_compliance(
            simulation_results, very_late_target, project_start_date
        )
        
        # Property: Very late target should have high completion probability
        assert late_result.completion_probability >= 0.95
        assert late_result.schedule_at_risk <= mean_duration * 0.1  # Low risk
    
    @given(
        schedule_outcomes_1=arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1000, max_value=3000),
            elements=st.floats(min_value=30.0, max_value=300.0, allow_nan=False, allow_infinity=False)
        ),
        schedule_outcomes_2=arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1000, max_value=3000),
            elements=st.floats(min_value=30.0, max_value=300.0, allow_nan=False, allow_infinity=False)
        )
    )
    @settings(max_examples=25, deadline=10000)
    def test_comparative_schedule_analysis(self, schedule_outcomes_1, schedule_outcomes_2):
        """
        Property: Comparative analysis should be consistent across different schedule distributions.
        
        This property ensures that when comparing different schedule distributions,
        the analysis produces consistent and logical results.
        """
        assume(len(schedule_outcomes_1) > 0 and len(schedule_outcomes_2) > 0)
        assume(np.all(np.isfinite(schedule_outcomes_1)) and np.all(np.isfinite(schedule_outcomes_2)))
        assume(np.all(schedule_outcomes_1 > 0) and np.all(schedule_outcomes_2 > 0))
        
        mean_1 = float(np.mean(schedule_outcomes_1))
        mean_2 = float(np.mean(schedule_outcomes_2))
        
        # Only test when means are sufficiently different
        assume(abs(mean_1 - mean_2) > min(mean_1, mean_2) * 0.1)
        
        # Create dates
        project_start_date = datetime(2024, 1, 1)
        # Use common target date
        target_date = project_start_date + timedelta(days=(mean_1 + mean_2) / 2)
        
        # Create simulation results
        simulation_results_1 = self._create_simulation_results(schedule_outcomes_1)
        simulation_results_2 = self._create_simulation_results(schedule_outcomes_2)
        
        generator = DistributionOutputGenerator()
        
        result_1 = generator.calculate_schedule_compliance(
            simulation_results_1, target_date, project_start_date
        )
        result_2 = generator.calculate_schedule_compliance(
            simulation_results_2, target_date, project_start_date
        )
        
        # Property: Distribution with shorter mean duration should have higher completion probability
        if mean_1 < mean_2:
            assert result_1.completion_probability >= result_2.completion_probability
            assert result_1.schedule_at_risk <= result_2.schedule_at_risk
        else:
            assert result_2.completion_probability >= result_1.completion_probability
            assert result_2.schedule_at_risk <= result_1.schedule_at_risk
    
    @given(
        schedule_outcomes=arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1000, max_value=5000),
            elements=st.floats(min_value=30.0, max_value=500.0, allow_nan=False, allow_infinity=False)
        )
    )
    @settings(max_examples=50, deadline=5000)
    def test_schedule_distribution_summary_consistency(self, schedule_outcomes):
        """
        Property: Schedule distribution summary statistics should be internally consistent.
        
        This property ensures that summary statistics are mathematically
        consistent and reasonable for schedule data.
        """
        assume(len(schedule_outcomes) > 0)
        assume(np.all(np.isfinite(schedule_outcomes)))
        assume(np.all(schedule_outcomes > 0))
        assume(np.std(schedule_outcomes) > 0)  # Ensure some variation
        
        generator = DistributionOutputGenerator()
        summary = generator.generate_distribution_summary(schedule_outcomes)
        
        # Property: Basic statistics should be reasonable
        assert summary.mean > 0
        assert summary.median > 0
        assert summary.std_dev >= 0
        assert np.isfinite(summary.mean)
        assert np.isfinite(summary.median)
        assert np.isfinite(summary.std_dev)
        
        # Property: Coefficient of variation should be reasonable for schedule data
        if summary.mean > 0:
            expected_cv = summary.std_dev / summary.mean
            assert abs(summary.coefficient_of_variation - expected_cv) < 1e-6
            # Schedule CV should typically be less than 1.0 (100%)
            assert summary.coefficient_of_variation <= 2.0  # Allow some flexibility
        
        # Property: Percentiles should be ordered and within data range
        percentiles = summary.percentiles
        p_values = [percentiles[f'P{p}'] for p in [10, 25, 50, 75, 90, 95, 99]]
        
        for i in range(len(p_values) - 1):
            assert p_values[i] <= p_values[i + 1]
        
        # Property: Percentiles should be within data range
        data_min = float(np.min(schedule_outcomes))
        data_max = float(np.max(schedule_outcomes))
        
        for p_value in p_values:
            assert data_min <= p_value <= data_max
        
        # Property: Median should match P50
        assert abs(summary.median - percentiles['P50']) < 1e-6
    
    @given(
        schedule_outcomes=arrays(
            dtype=np.float64,
            shape=st.integers(min_value=1000, max_value=5000),
            elements=st.floats(min_value=30.0, max_value=500.0, allow_nan=False, allow_infinity=False)
        )
    )
    @settings(max_examples=50, deadline=5000)
    def test_schedule_value_at_risk_properties(self, schedule_outcomes):
        """
        Property: Schedule Value at Risk calculations should have correct properties.
        
        This property ensures that VaR and CVaR calculations for schedule data
        are mathematically correct and consistent.
        """
        assume(len(schedule_outcomes) > 0)
        assume(np.all(np.isfinite(schedule_outcomes)))
        assume(np.all(schedule_outcomes > 0))
        
        generator = DistributionOutputGenerator()
        
        # Calculate VaR and CVaR for schedule outcomes
        var_results = generator.calculate_value_at_risk(schedule_outcomes, [0.95, 0.99])
        cvar_results = generator.calculate_conditional_value_at_risk(schedule_outcomes, [0.95, 0.99])
        
        # Property: VaR should be within data range
        data_min = float(np.min(schedule_outcomes))
        data_max = float(np.max(schedule_outcomes))
        
        for var_value in var_results.values():
            assert data_min <= var_value <= data_max
        
        # Property: CVaR should be >= VaR for same confidence level
        assert cvar_results['CVaR_95%'] >= var_results['VaR_95%']
        assert cvar_results['CVaR_99%'] >= var_results['VaR_99%']
        
        # Property: Higher confidence VaR should be >= lower confidence VaR
        assert var_results['VaR_99%'] >= var_results['VaR_95%']
        assert cvar_results['CVaR_99%'] >= cvar_results['CVaR_95%']
        
        # Property: Schedule VaR values should be reasonable (positive durations)
        for var_value in var_results.values():
            assert var_value > 0
        for cvar_value in cvar_results.values():
            assert cvar_value > 0
    
    def _create_simulation_results(self, schedule_outcomes: np.ndarray) -> SimulationResults:
        """Helper method to create SimulationResults for testing."""
        cost_outcomes = np.random.uniform(10000, 100000, len(schedule_outcomes))
        
        return SimulationResults(
            simulation_id="test_simulation",
            timestamp=datetime.now(),
            iteration_count=len(schedule_outcomes),
            cost_outcomes=cost_outcomes,
            schedule_outcomes=schedule_outcomes,
            risk_contributions={},
            convergence_metrics=ConvergenceMetrics(
                mean_stability=0.95,
                variance_stability=0.95,
                percentile_stability={50.0: 0.95, 90.0: 0.95},
                converged=True,
                iterations_to_convergence=len(schedule_outcomes)
            ),
            execution_time=1.0
        )