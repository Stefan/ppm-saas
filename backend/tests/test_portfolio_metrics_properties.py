"""
Property-based tests for Portfolio Metrics Performance
Feature: ai-ppm-platform, Property 1: Portfolio Metrics Calculation Performance
Validates: Requirements 1.2
"""

import pytest
from hypothesis import given, strategies as st, assume, settings, HealthCheck
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from unittest.mock import Mock, patch, MagicMock
from datetime import date, datetime, timedelta
import uuid
import json
import time

# Mock the Supabase client to avoid database dependencies
class MockSupabaseResponse:
    def __init__(self, data=None, count=0):
        self.data = data
        self.count = count

class MockSupabaseTable:
    def __init__(self):
        self.data_store = {}
        self.next_id = 1
        self.query_filters = {}
        self.update_data = None
    
    def insert(self, data):
        # Simulate database insert
        record = data.copy()
        record['id'] = str(uuid.uuid4())
        record['created_at'] = datetime.now().isoformat()
        record['updated_at'] = datetime.now().isoformat()
        
        self.data_store[record['id']] = record
        return MockSupabaseResponse([record])
    
    def select(self, fields):
        return self
    
    def eq(self, field, value):
        # Store filter for later execution
        self.query_filters[field] = value
        return self
    
    def update(self, data):
        self.update_data = data
        return self
    
    def delete(self):
        return self
    
    def execute(self):
        # Apply filters and return results
        if self.update_data:
            # Handle update operation
            filtered_records = []
            for record in self.data_store.values():
                matches = True
                for field, value in self.query_filters.items():
                    if record.get(field) != value:
                        matches = False
                        break
                if matches:
                    record.update(self.update_data)
                    record['updated_at'] = datetime.now().isoformat()
                    filtered_records.append(record)
            
            # Reset state
            self.update_data = None
            self.query_filters = {}
            return MockSupabaseResponse(filtered_records)
        
        # Handle select operation
        filtered_data = []
        for record in self.data_store.values():
            matches = True
            for field, value in self.query_filters.items():
                if record.get(field) != value:
                    matches = False
                    break
            if matches:
                filtered_data.append(record)
        
        # Reset filters
        self.query_filters = {}
        return MockSupabaseResponse(filtered_data)

class MockSupabaseClient:
    def __init__(self):
        self.tables = {
            'portfolios': MockSupabaseTable(),
            'projects': MockSupabaseTable(),
            'resources': MockSupabaseTable(),
            'risks': MockSupabaseTable(),
            'issues': MockSupabaseTable(),
            'milestones': MockSupabaseTable()
        }
    
    def table(self, table_name):
        return self.tables.get(table_name, MockSupabaseTable())

# Portfolio metrics calculation functions (implementing the logic that should exist in main.py)
def calculate_portfolio_metrics(portfolio_id, projects=None, mock_client=None):
    """
    Calculate comprehensive portfolio metrics from individual projects
    This function should complete within 5 seconds regardless of portfolio size up to 1000 projects
    """
    start_time = time.time()
    
    if projects is None:
        projects = []
    
    # Basic metrics
    total_projects = len(projects)
    active_projects = len([p for p in projects if p.get('status') == 'active'])
    completed_projects = len([p for p in projects if p.get('status') == 'completed'])
    on_hold_projects = len([p for p in projects if p.get('status') == 'on-hold'])
    cancelled_projects = len([p for p in projects if p.get('status') == 'cancelled'])
    
    # Health distribution
    health_distribution = {
        'green': len([p for p in projects if p.get('health') == 'green']),
        'yellow': len([p for p in projects if p.get('health') == 'yellow']),
        'red': len([p for p in projects if p.get('health') == 'red'])
    }
    
    # Financial metrics
    total_budget = sum(float(p.get('budget', 0)) for p in projects if p.get('budget'))
    total_actual_cost = sum(float(p.get('actual_cost', 0)) for p in projects if p.get('actual_cost'))
    budget_variance = total_actual_cost - total_budget
    budget_variance_percentage = (budget_variance / total_budget * 100) if total_budget > 0 else 0
    
    # Resource utilization (simplified calculation)
    total_team_members = sum(len(p.get('team_members', [])) for p in projects)
    avg_team_size = total_team_members / total_projects if total_projects > 0 else 0
    
    # Schedule performance
    overdue_projects = 0
    on_schedule_projects = 0
    today = date.today()
    
    for project in projects:
        if project.get('end_date') and project.get('status') != 'completed':
            try:
                end_date = datetime.fromisoformat(project['end_date']).date() if isinstance(project['end_date'], str) else project['end_date']
                if end_date < today:
                    overdue_projects += 1
                else:
                    on_schedule_projects += 1
            except (ValueError, TypeError):
                # Handle invalid date formats gracefully
                pass
    
    # Risk assessment
    high_risk_projects = len([p for p in projects if p.get('health') == 'red'])
    medium_risk_projects = len([p for p in projects if p.get('health') == 'yellow'])
    low_risk_projects = len([p for p in projects if p.get('health') == 'green'])
    
    # Portfolio health score (weighted average)
    if total_projects > 0:
        health_score = (
            (low_risk_projects * 100) + 
            (medium_risk_projects * 60) + 
            (high_risk_projects * 20)
        ) / total_projects
    else:
        health_score = 100
    
    # Completion rate
    completion_rate = (completed_projects / total_projects * 100) if total_projects > 0 else 0
    
    # Success rate (completed projects that were on budget and on time)
    successful_projects = 0
    for project in projects:
        if project.get('status') == 'completed':
            budget_ok = True
            schedule_ok = True
            
            # Check budget performance
            if project.get('budget') and project.get('actual_cost'):
                budget_variance_pct = ((float(project['actual_cost']) - float(project['budget'])) / float(project['budget'])) * 100
                budget_ok = budget_variance_pct <= 10  # Within 10% of budget
            
            # Check schedule performance (simplified - would need more data in real implementation)
            if project.get('end_date'):
                try:
                    end_date = datetime.fromisoformat(project['end_date']).date() if isinstance(project['end_date'], str) else project['end_date']
                    # Assume project was completed on time if end date is not in the past by more than 30 days
                    schedule_ok = (today - end_date).days <= 30
                except (ValueError, TypeError):
                    schedule_ok = True  # Default to OK if we can't parse date
            
            if budget_ok and schedule_ok:
                successful_projects += 1
    
    success_rate = (successful_projects / completed_projects * 100) if completed_projects > 0 else 0
    
    # Calculate execution time
    execution_time = time.time() - start_time
    
    return {
        'portfolio_id': portfolio_id,
        'total_projects': total_projects,
        'active_projects': active_projects,
        'completed_projects': completed_projects,
        'on_hold_projects': on_hold_projects,
        'cancelled_projects': cancelled_projects,
        'health_distribution': health_distribution,
        'financial_metrics': {
            'total_budget': total_budget,
            'total_actual_cost': total_actual_cost,
            'budget_variance': budget_variance,
            'budget_variance_percentage': budget_variance_percentage
        },
        'resource_metrics': {
            'total_team_members': total_team_members,
            'avg_team_size': avg_team_size
        },
        'schedule_metrics': {
            'overdue_projects': overdue_projects,
            'on_schedule_projects': on_schedule_projects,
            'completion_rate': completion_rate
        },
        'risk_metrics': {
            'high_risk_projects': high_risk_projects,
            'medium_risk_projects': medium_risk_projects,
            'low_risk_projects': low_risk_projects,
            'portfolio_health_score': health_score
        },
        'performance_metrics': {
            'success_rate': success_rate,
            'successful_projects': successful_projects
        },
        'execution_time_seconds': execution_time,
        'calculated_at': datetime.now().isoformat()
    }

# Test data strategies for property-based testing
@st.composite
def portfolio_strategy(draw):
    """Generate valid portfolio data for testing"""
    return {
        "id": str(uuid.uuid4()),
        "name": draw(st.text(min_size=1, max_size=100)),
        "description": draw(st.one_of(st.none(), st.text(max_size=500))),
        "owner_id": str(uuid.uuid4()),
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat()
    }

@st.composite
def project_strategy(draw):
    """Generate valid project data for testing"""
    statuses = ['planning', 'active', 'on-hold', 'completed', 'cancelled']
    health_indicators = ['green', 'yellow', 'red']
    
    start_date = draw(st.dates(min_value=date(2020, 1, 1), max_value=date(2025, 12, 31)))
    end_date = draw(st.dates(min_value=start_date, max_value=date(2026, 12, 31)))
    
    budget = draw(st.floats(min_value=1000.0, max_value=1000000.0, allow_nan=False, allow_infinity=False))
    actual_cost = draw(st.floats(min_value=0.0, max_value=budget * 2, allow_nan=False, allow_infinity=False))
    
    return {
        "id": str(uuid.uuid4()),
        "portfolio_id": str(uuid.uuid4()),
        "name": draw(st.text(min_size=1, max_size=100)),
        "description": draw(st.one_of(st.none(), st.text(max_size=500))),
        "status": draw(st.sampled_from(statuses)),
        "health": draw(st.sampled_from(health_indicators)),
        "priority": draw(st.one_of(st.none(), st.sampled_from(['low', 'medium', 'high', 'critical']))),
        "budget": budget,
        "actual_cost": actual_cost,
        "start_date": start_date.isoformat(),
        "end_date": end_date.isoformat(),
        "manager_id": str(uuid.uuid4()),
        "team_members": [str(uuid.uuid4()) for _ in range(draw(st.integers(min_value=0, max_value=10)))],
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat()
    }

@st.composite
def large_portfolio_strategy(draw):
    """Generate portfolios with many projects for performance testing"""
    portfolio = draw(portfolio_strategy())
    num_projects = draw(st.integers(min_value=10, max_value=1000))  # Up to 1000 projects as per requirement
    
    projects = []
    for _ in range(num_projects):
        project = draw(project_strategy())
        project["portfolio_id"] = portfolio["id"]
        projects.append(project)
    
    return portfolio, projects

class TestPortfolioMetricsCalculationPerformance:
    """Property 1: Portfolio Metrics Calculation Performance tests"""

    @settings(max_examples=20, deadline=10000, suppress_health_check=[HealthCheck.data_too_large])  # 10 second deadline for performance testing
    @given(portfolio_data=large_portfolio_strategy())
    def test_portfolio_metrics_calculation_performance(self, portfolio_data):
        """
        Property 1: Portfolio Metrics Calculation Performance
        For any portfolio with projects, calculating portfolio metrics should complete within 5 seconds regardless of portfolio size up to 1000 projects
        Validates: Requirements 1.2
        """
        portfolio, projects = portfolio_data
        portfolio_id = portfolio["id"]
        
        # Measure calculation time
        start_time = time.time()
        metrics = calculate_portfolio_metrics(portfolio_id, projects)
        execution_time = time.time() - start_time
        
        # Verify performance requirement: should complete within 5 seconds
        assert execution_time <= 5.0, f"Portfolio metrics calculation took {execution_time:.2f} seconds, should be <= 5.0 seconds for {len(projects)} projects"
        
        # Verify metrics are calculated correctly
        assert metrics is not None, "Metrics calculation should return results"
        assert metrics['portfolio_id'] == portfolio_id, "Metrics should include correct portfolio ID"
        assert metrics['total_projects'] == len(projects), f"Total projects should be {len(projects)}, got {metrics['total_projects']}"
        
        # Verify execution time is recorded in metrics
        assert 'execution_time_seconds' in metrics, "Metrics should include execution time"
        assert metrics['execution_time_seconds'] <= 5.0, "Recorded execution time should be within performance limit"

    @settings(max_examples=50)
    @given(
        portfolio=portfolio_strategy(),
        projects=st.lists(project_strategy(), min_size=1, max_size=100)
    )
    def test_metrics_calculation_accuracy(self, portfolio, projects):
        """
        Property 1: Portfolio Metrics Calculation Performance - Accuracy
        For any portfolio with projects, calculated metrics should accurately reflect project data
        Validates: Requirements 1.2
        """
        portfolio_id = portfolio["id"]
        
        # Link all projects to the portfolio
        for project in projects:
            project["portfolio_id"] = portfolio_id
        
        # Calculate metrics
        metrics = calculate_portfolio_metrics(portfolio_id, projects)
        
        # Verify basic counts
        assert metrics['total_projects'] == len(projects), "Total projects count should be accurate"
        
        # Verify status distribution
        expected_active = len([p for p in projects if p.get('status') == 'active'])
        expected_completed = len([p for p in projects if p.get('status') == 'completed'])
        expected_on_hold = len([p for p in projects if p.get('status') == 'on-hold'])
        expected_cancelled = len([p for p in projects if p.get('status') == 'cancelled'])
        
        assert metrics['active_projects'] == expected_active, f"Active projects count should be {expected_active}, got {metrics['active_projects']}"
        assert metrics['completed_projects'] == expected_completed, f"Completed projects count should be {expected_completed}, got {metrics['completed_projects']}"
        assert metrics['on_hold_projects'] == expected_on_hold, f"On-hold projects count should be {expected_on_hold}, got {metrics['on_hold_projects']}"
        assert metrics['cancelled_projects'] == expected_cancelled, f"Cancelled projects count should be {expected_cancelled}, got {metrics['cancelled_projects']}"
        
        # Verify health distribution
        expected_green = len([p for p in projects if p.get('health') == 'green'])
        expected_yellow = len([p for p in projects if p.get('health') == 'yellow'])
        expected_red = len([p for p in projects if p.get('health') == 'red'])
        
        assert metrics['health_distribution']['green'] == expected_green, f"Green health count should be {expected_green}"
        assert metrics['health_distribution']['yellow'] == expected_yellow, f"Yellow health count should be {expected_yellow}"
        assert metrics['health_distribution']['red'] == expected_red, f"Red health count should be {expected_red}"
        
        # Verify financial calculations
        expected_budget = sum(float(p.get('budget', 0)) for p in projects if p.get('budget'))
        expected_actual = sum(float(p.get('actual_cost', 0)) for p in projects if p.get('actual_cost'))
        
        assert abs(metrics['financial_metrics']['total_budget'] - expected_budget) < 0.01, "Total budget should be calculated correctly"
        assert abs(metrics['financial_metrics']['total_actual_cost'] - expected_actual) < 0.01, "Total actual cost should be calculated correctly"
        
        # Verify variance calculation
        expected_variance = expected_actual - expected_budget
        assert abs(metrics['financial_metrics']['budget_variance'] - expected_variance) < 0.01, "Budget variance should be calculated correctly"

    @settings(max_examples=10, suppress_health_check=[HealthCheck.large_base_example, HealthCheck.too_slow, HealthCheck.data_too_large])
    @given(
        portfolio=portfolio_strategy(),
        projects=st.lists(project_strategy(), min_size=100, max_size=500)
    )
    def test_performance_scales_with_project_count(self, portfolio, projects):
        """
        Property 1: Portfolio Metrics Calculation Performance - Scalability
        For any portfolio, calculation time should scale reasonably with project count
        Validates: Requirements 1.2
        """
        portfolio_id = portfolio["id"]
        
        # Link all projects to the portfolio
        for project in projects:
            project["portfolio_id"] = portfolio_id
        
        # Test with different subset sizes to verify scaling
        subset_sizes = [50, 100, len(projects)]
        execution_times = []
        
        for size in subset_sizes:
            if size <= len(projects):
                project_subset = projects[:size]
                
                start_time = time.time()
                metrics = calculate_portfolio_metrics(portfolio_id, project_subset)
                execution_time = time.time() - start_time
                execution_times.append((size, execution_time))
                
                # Each calculation should still be within performance limit
                assert execution_time <= 5.0, f"Calculation with {size} projects took {execution_time:.2f} seconds, should be <= 5.0"
        
        # Verify that calculation time doesn't grow exponentially
        if len(execution_times) >= 2:
            # Time should not more than double when project count doubles
            for i in range(1, len(execution_times)):
                prev_size, prev_time = execution_times[i-1]
                curr_size, curr_time = execution_times[i]
                
                # If times are very small (< 0.001s), skip scaling check as it's not meaningful
                if prev_time >= 0.001 and curr_time >= 0.001:
                    time_ratio = curr_time / prev_time
                    size_ratio = curr_size / prev_size
                    
                    # Time growth should be reasonable (not exponential)
                    # Allow some flexibility for small variations in timing
                    assert time_ratio <= size_ratio * 3, f"Performance should scale reasonably: {prev_size} projects in {prev_time:.3f}s, {curr_size} projects in {curr_time:.3f}s"

    @settings(max_examples=30)
    @given(
        portfolio=portfolio_strategy(),
        projects=st.lists(project_strategy(), min_size=1, max_size=50)
    )
    def test_metrics_consistency_across_calculations(self, portfolio, projects):
        """
        Property 1: Portfolio Metrics Calculation Performance - Consistency
        For any portfolio, multiple calculations should produce consistent results
        Validates: Requirements 1.2
        """
        portfolio_id = portfolio["id"]
        
        # Link all projects to the portfolio
        for project in projects:
            project["portfolio_id"] = portfolio_id
        
        # Calculate metrics multiple times
        metrics1 = calculate_portfolio_metrics(portfolio_id, projects)
        metrics2 = calculate_portfolio_metrics(portfolio_id, projects)
        metrics3 = calculate_portfolio_metrics(portfolio_id, projects)
        
        # All calculations should produce the same results (excluding timestamps and execution times)
        assert metrics1['total_projects'] == metrics2['total_projects'] == metrics3['total_projects'], "Total projects should be consistent"
        assert metrics1['active_projects'] == metrics2['active_projects'] == metrics3['active_projects'], "Active projects should be consistent"
        assert metrics1['completed_projects'] == metrics2['completed_projects'] == metrics3['completed_projects'], "Completed projects should be consistent"
        
        # Financial metrics should be consistent
        assert abs(metrics1['financial_metrics']['total_budget'] - metrics2['financial_metrics']['total_budget']) < 0.01, "Budget calculations should be consistent"
        assert abs(metrics2['financial_metrics']['total_budget'] - metrics3['financial_metrics']['total_budget']) < 0.01, "Budget calculations should be consistent"
        
        # Health distribution should be consistent
        assert metrics1['health_distribution'] == metrics2['health_distribution'] == metrics3['health_distribution'], "Health distribution should be consistent"
        
        # All calculations should complete within performance limit
        assert metrics1['execution_time_seconds'] <= 5.0, "First calculation should be within performance limit"
        assert metrics2['execution_time_seconds'] <= 5.0, "Second calculation should be within performance limit"
        assert metrics3['execution_time_seconds'] <= 5.0, "Third calculation should be within performance limit"

    def test_empty_portfolio_performance(self):
        """
        Property 1: Portfolio Metrics Calculation Performance - Edge Case
        For any empty portfolio, metrics calculation should complete quickly and handle gracefully
        Validates: Requirements 1.2
        """
        portfolio_id = str(uuid.uuid4())
        empty_projects = []
        
        start_time = time.time()
        metrics = calculate_portfolio_metrics(portfolio_id, empty_projects)
        execution_time = time.time() - start_time
        
        # Should complete very quickly for empty portfolio
        assert execution_time <= 1.0, f"Empty portfolio calculation took {execution_time:.2f} seconds, should be very fast"
        
        # Should handle empty portfolio gracefully
        assert metrics['total_projects'] == 0, "Empty portfolio should have 0 total projects"
        assert metrics['active_projects'] == 0, "Empty portfolio should have 0 active projects"
        assert metrics['financial_metrics']['total_budget'] == 0, "Empty portfolio should have 0 total budget"
        assert metrics['health_distribution']['green'] == 0, "Empty portfolio should have 0 green projects"
        assert metrics['health_distribution']['yellow'] == 0, "Empty portfolio should have 0 yellow projects"
        assert metrics['health_distribution']['red'] == 0, "Empty portfolio should have 0 red projects"

    def test_maximum_portfolio_size_performance(self):
        """
        Property 1: Portfolio Metrics Calculation Performance - Maximum Size
        For any portfolio with up to 1000 projects, calculation should complete within 5 seconds
        Validates: Requirements 1.2
        """
        portfolio_id = str(uuid.uuid4())
        
        # Create a deterministic large dataset for performance testing
        projects = []
        for i in range(1000):  # Test with exactly 1000 projects
            project = {
                "id": str(uuid.uuid4()),
                "portfolio_id": portfolio_id,
                "name": f"Test Project {i}",
                "description": f"Description for project {i}",
                "status": ["planning", "active", "on-hold", "completed", "cancelled"][i % 5],
                "health": ["green", "yellow", "red"][i % 3],
                "priority": ["low", "medium", "high", "critical"][i % 4],
                "budget": 10000.0 + (i * 100),
                "actual_cost": 8000.0 + (i * 80),
                "start_date": (date(2024, 1, 1) + timedelta(days=i % 365)).isoformat(),
                "end_date": (date(2024, 6, 1) + timedelta(days=i % 365)).isoformat(),
                "manager_id": str(uuid.uuid4()),
                "team_members": [str(uuid.uuid4()) for _ in range(i % 5)],
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat()
            }
            projects.append(project)
        
        # This is the critical test for the performance requirement
        start_time = time.time()
        metrics = calculate_portfolio_metrics(portfolio_id, projects)
        execution_time = time.time() - start_time
        
        # Must complete within 5 seconds even for maximum portfolio size
        assert execution_time <= 5.0, f"Large portfolio calculation with {len(projects)} projects took {execution_time:.2f} seconds, must be <= 5.0 seconds"
        
        # Verify all metrics are calculated correctly even for large portfolios
        assert metrics['total_projects'] == len(projects), f"Large portfolio should correctly count {len(projects)} projects"
        assert 'financial_metrics' in metrics, "Large portfolio should include financial metrics"
        assert 'health_distribution' in metrics, "Large portfolio should include health distribution"
        assert 'risk_metrics' in metrics, "Large portfolio should include risk metrics"
        
        # Verify performance is recorded
        assert metrics['execution_time_seconds'] <= 5.0, "Recorded execution time should meet performance requirement"

if __name__ == "__main__":
    # Run the tests
    pytest.main([__file__, "-v"])