/**
 * Property-Based Test: Web Vitals Tracking
 * Feature: admin-performance-optimization
 * Property 17: Web Vitals Measured and Reported
 * **Validates: Requirements 9.1, 9.2**
 * 
 * This test verifies that the usePerformanceMonitoring hook properly
 * measures Core Web Vitals (LCP, FID, CLS, TTFB, INP, FCP) and sends
 * them to the analytics endpoint.
 */

import { renderHook, waitFor, cleanup } from '@testing-library/react'
import fc from 'fast-check'
import { usePerformanceMonitoring } from '../hooks/usePerformanceMonitoring'

// Mock web-vitals library
jest.mock('web-vitals', () => ({
  onCLS: jest.fn((callback) => {
    // Simulate CLS metric
    callback({ name: 'CLS', value: 0.05, rating: 'good', delta: 0.05, id: 'test-cls' })
  }),
  onFID: jest.fn((callback) => {
    // Simulate FID metric
    callback({ name: 'FID', value: 50, rating: 'good', delta: 50, id: 'test-fid' })
  }),
  onLCP: jest.fn((callback) => {
    // Simulate LCP metric
    callback({ name: 'LCP', value: 2000, rating: 'good', delta: 2000, id: 'test-lcp' })
  }),
  onTTFB: jest.fn((callback) => {
    // Simulate TTFB metric
    callback({ name: 'TTFB', value: 500, rating: 'good', delta: 500, id: 'test-ttfb' })
  }),
  onINP: jest.fn((callback) => {
    // Simulate INP metric
    callback({ name: 'INP', value: 150, rating: 'good', delta: 150, id: 'test-inp' })
  }),
  onFCP: jest.fn((callback) => {
    // Simulate FCP metric
    callback({ name: 'FCP', value: 1200, rating: 'good', delta: 1200, id: 'test-fcp' })
  })
}))

// Mock fetch for analytics endpoint
global.fetch = jest.fn()

describe('Admin Performance Optimization - Web Vitals Tracking', () => {
  beforeEach(() => {
    jest.clearAllMocks()
    ;(global.fetch as jest.Mock).mockResolvedValue({
      ok: true,
      json: async () => ({ success: true })
    })
  })

  afterEach(() => {
    cleanup()
  })

  describe('Property 17: Web Vitals Measured and Reported', () => {
    it('should measure all Core Web Vitals metrics', async () => {
      await fc.assert(
        fc.asyncProperty(
          fc.record({
            enabled: fc.boolean(),
            reportInterval: fc.integer({ min: 1000, max: 60000 })
          }),
          async (options) => {
            const reports: any[] = []
            
            const { result, unmount } = renderHook(() =>
              usePerformanceMonitoring({
                enabled: options.enabled,
                reportInterval: options.reportInterval,
                onReport: (report) => reports.push(report)
              })
            )

            // Wait for web vitals to be captured
            await waitFor(() => {
              expect(result.current.isMonitoring).toBe(options.enabled)
            }, { timeout: 1000 })

            if (options.enabled) {
              // Generate a report
              const report = result.current.generateReport()

              // Verify all Core Web Vitals are present
              expect(report.webVitals).toBeDefined()
              expect(report.webVitals.lcp).toBeDefined()
              expect(report.webVitals.fid).toBeDefined()
              expect(report.webVitals.cls).toBeDefined()
              expect(report.webVitals.ttfb).toBeDefined()
              expect(report.webVitals.inp).toBeDefined()
              expect(report.webVitals.fcp).toBeDefined()

              // Verify metrics are within reasonable ranges
              if (report.webVitals.lcp) {
                expect(report.webVitals.lcp).toBeGreaterThan(0)
              }
              if (report.webVitals.fid) {
                expect(report.webVitals.fid).toBeGreaterThanOrEqual(0)
              }
              if (report.webVitals.cls) {
                expect(report.webVitals.cls).toBeGreaterThanOrEqual(0)
              }
              if (report.webVitals.ttfb) {
                expect(report.webVitals.ttfb).toBeGreaterThan(0)
              }
              if (report.webVitals.inp) {
                expect(report.webVitals.inp).toBeGreaterThanOrEqual(0)
              }
              if (report.webVitals.fcp) {
                expect(report.webVitals.fcp).toBeGreaterThan(0)
              }
            }

            unmount()
          }
        ),
        { numRuns: 100 }
      )
    })

    it('should send metrics to analytics endpoint', async () => {
      await fc.assert(
        fc.asyncProperty(
          fc.record({
            analyticsEndpoint: fc.constantFrom(
              '/api/analytics/performance',
              '/api/metrics',
              '/api/tracking/vitals'
            ),
            reportInterval: fc.integer({ min: 100, max: 1000 })
          }),
          async (options) => {
            const { result, unmount } = renderHook(() =>
              usePerformanceMonitoring({
                enabled: true,
                analyticsEndpoint: options.analyticsEndpoint,
                reportInterval: options.reportInterval
              })
            )

            // Wait for monitoring to start
            await waitFor(() => {
              expect(result.current.isMonitoring).toBe(true)
            }, { timeout: 1000 })

            // Manually trigger report
            await result.current.sendReport()

            // Wait for fetch to be called
            await waitFor(() => {
              expect(global.fetch).toHaveBeenCalled()
            }, { timeout: 2000 })

            // Verify fetch was called with correct endpoint
            const fetchCalls = (global.fetch as jest.Mock).mock.calls
            const analyticsCall = fetchCalls.find(
              call => call[0] === options.analyticsEndpoint
            )

            if (analyticsCall) {
              expect(analyticsCall[0]).toBe(options.analyticsEndpoint)
              expect(analyticsCall[1].method).toBe('POST')
              expect(analyticsCall[1].headers['Content-Type']).toBe('application/json')

              // Verify payload contains web vitals
              const payload = JSON.parse(analyticsCall[1].body)
              expect(payload.webVitals).toBeDefined()
              expect(payload.timestamp).toBeDefined()
            }

            unmount()
          }
        ),
        { numRuns: 50 }
      )
    })

    it('should include all required fields in analytics payload', async () => {
      await fc.assert(
        fc.asyncProperty(
          fc.constant({}),
          async () => {
            const { result, unmount } = renderHook(() =>
              usePerformanceMonitoring({
                enabled: true,
                analyticsEndpoint: '/api/analytics/performance'
              })
            )

            await waitFor(() => {
              expect(result.current.isMonitoring).toBe(true)
            }, { timeout: 1000 })

            // Send report
            await result.current.sendReport()

            await waitFor(() => {
              expect(global.fetch).toHaveBeenCalled()
            }, { timeout: 2000 })

            const fetchCalls = (global.fetch as jest.Mock).mock.calls
            if (fetchCalls.length > 0) {
              const lastCall = fetchCalls[fetchCalls.length - 1]
              const payload = JSON.parse(lastCall[1].body)

              // Verify required fields
              expect(payload).toHaveProperty('webVitals')
              expect(payload).toHaveProperty('metrics')
              expect(payload).toHaveProperty('resourceTiming')
              expect(payload).toHaveProperty('customMetrics')
              expect(payload).toHaveProperty('longTasks')
              expect(payload).toHaveProperty('timestamp')
              expect(payload).toHaveProperty('url')
              expect(payload).toHaveProperty('userAgent')
            }

            unmount()
          }
        ),
        { numRuns: 50 }
      )
    })

    it('should handle analytics endpoint failures gracefully', async () => {
      await fc.assert(
        fc.asyncProperty(
          fc.constantFrom(
            { ok: false, status: 500 },
            { ok: false, status: 404 },
            { ok: false, status: 403 }
          ),
          async (errorResponse) => {
            // Mock fetch to fail
            ;(global.fetch as jest.Mock).mockResolvedValueOnce(errorResponse)

            const consoleErrorSpy = jest.spyOn(console, 'error').mockImplementation()

            const { result, unmount } = renderHook(() =>
              usePerformanceMonitoring({
                enabled: true,
                analyticsEndpoint: '/api/analytics/performance'
              })
            )

            await waitFor(() => {
              expect(result.current.isMonitoring).toBe(true)
            }, { timeout: 1000 })

            // Send report - should not throw
            await expect(result.current.sendReport()).resolves.not.toThrow()

            consoleErrorSpy.mockRestore()
            unmount()
          }
        ),
        { numRuns: 50 }
      )
    })

    it('should track metrics over time with periodic reporting', async () => {
      await fc.assert(
        fc.asyncProperty(
          fc.integer({ min: 100, max: 500 }),
          async (reportInterval) => {
            const reports: any[] = []

            const { result, unmount } = renderHook(() =>
              usePerformanceMonitoring({
                enabled: true,
                reportInterval,
                onReport: (report) => reports.push(report)
              })
            )

            await waitFor(() => {
              expect(result.current.isMonitoring).toBe(true)
            }, { timeout: 1000 })

            // Record some custom metrics
            result.current.recordCustomMetric('test_metric_1', 100)
            result.current.recordCustomMetric('test_metric_2', 200)

            // Wait for at least one report interval
            await new Promise(resolve => setTimeout(resolve, reportInterval + 100))

            // Verify reports were generated
            // Note: Due to timing, we may or may not have reports yet
            // The important thing is that the hook doesn't crash
            expect(result.current.isMonitoring).toBe(true)

            unmount()
          }
        ),
        { numRuns: 30 }
      )
    })

    it('should properly cleanup on unmount', async () => {
      await fc.assert(
        fc.asyncProperty(
          fc.integer({ min: 1000, max: 5000 }),
          async (reportInterval) => {
            const { result, unmount } = renderHook(() =>
              usePerformanceMonitoring({
                enabled: true,
                reportInterval,
                analyticsEndpoint: '/api/analytics/performance'
              })
            )

            await waitFor(() => {
              expect(result.current.isMonitoring).toBe(true)
            }, { timeout: 1000 })

            const initialFetchCount = (global.fetch as jest.Mock).mock.calls.length

            // Unmount should trigger final report
            unmount()

            // Wait a bit to ensure cleanup happens
            await new Promise(resolve => setTimeout(resolve, 100))

            // Verify no more reports are sent after unmount
            const finalFetchCount = (global.fetch as jest.Mock).mock.calls.length
            
            // Should have sent final report on unmount
            expect(finalFetchCount).toBeGreaterThanOrEqual(initialFetchCount)

            // Wait longer than report interval to ensure no more reports
            await new Promise(resolve => setTimeout(resolve, reportInterval + 100))
            
            const afterWaitFetchCount = (global.fetch as jest.Mock).mock.calls.length
            
            // No additional reports should be sent after unmount
            expect(afterWaitFetchCount).toBe(finalFetchCount)
          }
        ),
        { numRuns: 30 }
      )
    })
  })
})
